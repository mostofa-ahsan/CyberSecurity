{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b02c4a7c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'matplotlib' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6156/672832118.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m         'size'   : 22}\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'font'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfont\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;31m#from google.colab import files\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m#uploaded = files.upload() # If you run this on colab you need to upload the spam.csv file to colab\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'matplotlib' is not defined"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import torch\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "font = {'family' : 'normal',\n",
    "        'weight' : 'bold',\n",
    "        'size'   : 22}\n",
    "\n",
    "matplotlib.rc('font', **font)\n",
    "#from google.colab import files\n",
    "#uploaded = files.upload() # If you run this on colab you need to upload the spam.csv file to colab\n",
    "\n",
    "# Loading the dataset\n",
    "df = pd.read_csv(r'C:\\Users\\mkahs\\Repository\\SPAM-BERT\\SPAM text message 20170820 - Data.csv', encoding='latin-1')\n",
    "\n",
    "# Dropping unwanted columns\n",
    "# df.drop([\"Unnamed: 2\", \"Unnamed: 3\", \"Unnamed: 4\"], inplace=True, axis=1)\n",
    "\n",
    "# Chaning the labels for convinience\n",
    "df[\"Category\"].replace({\"ham\": 0, \"spam\":1}, inplace=True)\n",
    "\n",
    "# Changing the column names for better \n",
    "df.rename({\"Category\": \"spam\", \"Message\": \"original_message\"},axis=1, inplace=True)\n",
    "\n",
    "\n",
    "#import nltk\n",
    "#nltk.download(\"stopwords\")\n",
    "#nltk.download(\"punkt\") # again if your running this in colab you'll probably need to first download the stopwords set and punkt from nltk\n",
    "\n",
    "def clean_sentence(s):\n",
    "    \"\"\"Given a sentence remove its punctuation and stop words\"\"\"\n",
    "    \n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    s = s.translate(str.maketrans('','',string.punctuation)) # remove punctuation\n",
    "    tokens = word_tokenize(s)\n",
    "    cleaned_s = [w for w in tokens if w not in stop_words] # removing stop-words\n",
    "    return \" \".join(cleaned_s[:30]) # using the first 30 tokens only\n",
    "\n",
    "# Clean the sentences\n",
    "df[\"cleaned_message\"] = df[\"original_message\"].apply(clean_sentence)\n",
    "\n",
    "\n",
    "# Loading pretrained model/tokenizer\n",
    "# This is the Distilled, base, uncased version of BERT \n",
    "tokenizer = transformers.DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "model = transformers.DistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Tokenize the sentences adding the special \"[CLS]\" and \"[SEP]\" tokens\n",
    "tokenized = df[\"cleaned_message\"].apply(lambda x: tokenizer.encode(x, add_special_tokens=True))\n",
    "\n",
    "# Get the length of the longest tokenized sentence\n",
    "max_len = tokenized.apply(len).max() \n",
    "\n",
    "# Padd the rest of the sentence with zeros if the sentence is smaller than the longest sentence\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values]) \n",
    "\n",
    "# Create the attention mask so BERT knows to ignore the zeros used for padding\n",
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "\n",
    "# Create the input tensors\n",
    "input_ids = torch.tensor(padded)  \n",
    "attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "# Pass the inputs through DistilBERT\n",
    "with torch.no_grad():\n",
    "    encoder_hidden_state = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# Create a new dataframe with the encoded features\n",
    "df_encoded = pd.DataFrame(encoder_hidden_state[0][:,0,:].numpy())\n",
    "\n",
    "# Insert the original columns in the beginning of the encoded dataframe\n",
    "df_encoded.insert(loc=0, column='original_message', value=df[\"original_message\"])\n",
    "df_encoded.insert(loc=0, column='spam', value=df[\"spam\"])\n",
    "\n",
    "# Download the encoded csv\n",
    "df_encoded.to_csv(r'C:\\Users\\mkahs\\Repository\\SPAM-BERT\\Encoded_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1330a44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
